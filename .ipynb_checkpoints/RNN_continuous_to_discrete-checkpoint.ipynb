{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs: Continuous Input to Discrete (Classification) Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this notebook, we aim to input a sequence of numbers where a few fit a model and extract a sequence of 1s and 0s of the components of the sequence that best fit the model.\n",
    "\n",
    "In this example, we will try to train an RNN model that learns whether a number is greater than 0.5 or less than 0.5 and construct a corresponding sequence of 1s and 0s.\n",
    "\n",
    "Since RNNs are unrolled through time, it seems like there should be some relation between each node since the outputs of a current node are fed into the future nodes. In the current example, this is not implemented, but we will try to use an example that does in the end.\n",
    "\n",
    "<!---\n",
    "- With Bayesian networks, we model our data problem as a **causal network**, or a **story** involving **hidden** and **observed** variables.\n",
    "   - We come up with **a story** that we think **explains our data**.\n",
    "   - We **use Bayes's rule to find** _posterior_ probability distributions of the **hidden variables** given our observed variables (data).\n",
    "   - We use the full posterior distributions for our next action (decision, recommendation, prediction).\n",
    "- **Advantages**:\n",
    "   - **Confidence estimates.**\n",
    "   - **Flexibility.** We can change the story and re-train the model.\n",
    "     - Classical machine learning methods are **inflexible**: Code and theory only works for its specific problem.\n",
    "     - Your real world data probably has some important \n",
    "Difficult to adapt to your particular problem, which may not have been studied by researchers.\n",
    " - **Disadvantages**:\n",
    "     - **Slow** (unless you do a lot of math)\n",
    "     - **You have to think.**\n",
    "- **For many of your data problems, you may want or need to fit a custom machine learning model.**\n",
    "     - Some important aspect of your data probably hasn't been studied by researchers.\n",
    "     - I don't agree with this picture: http://scikit-learn.org/stable/_static/ml_map.png\n",
    "     -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data\n",
    "\n",
    "In this section, we generate a data sample of 1000x10 random numbers and 1000x10 corresponding sequeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5881308   0.89771373  0.89153073  0.81583748  0.03588959  0.69175758\n",
      "  0.37868094  0.51851095  0.65795147  0.19385022]\n",
      "[ 1.  1.  1.  1.  0.  1.  0.  1.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "seed = 20\n",
    "np.random.seed(seed)\n",
    "\n",
    "data = np.random.rand(1000,10)\n",
    "labels = np.zeros((1000,10))\n",
    "\n",
    "labels[np.where(data>.5)] = 1\n",
    "print(data[0])\n",
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Hyperparameters\n",
    "\n",
    "Now we set up the hyperparameters for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10                 # Sequence Length 10\n",
    "n_inputs = 1                 # Univariate, for now\n",
    "n_neurons = 100              # Set by trial and error\n",
    "n_outputs = 10               # Output sequence of length 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the RNN\n",
    "\n",
    "Here we set up the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Input Placeholders\n",
    "# Mini-batch size, sequence length, number of features\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "\n",
    "# Create a BasicRNNCell which gets copied to build the unrolled RNN\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "'''\n",
    "# Dynamic_rnn uses a while_loop to run the cell the appropirate number of times\n",
    "# Two inputs\n",
    "# 1. Cell\n",
    "# 2. Single tensor for all inputs at every time step (shape [None, n_steps, n_inputs])\n",
    "# Two outputs:\n",
    "# 1. List of output tensors for each time step\n",
    "# 2. Tensor containing the final stats of the network\n",
    "'''\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "'''\n",
    "## LAYERS\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "\n",
    "# Construct  output\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "\n",
    "print(rnn_outputs)\n",
    "print(states)\n",
    "'''\n",
    "\n",
    "# Training.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=rnn_outputs, labels=y)\n",
    "cross_entropy = tf.reduce_sum(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose:0\", shape=(?, 28, 150), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_2:0\", shape=(?, 150), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reset_graph()\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "print(outputs)\n",
    "print(states)\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "print(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "print(xentropy)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
